{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMyQ0RFimEUb",
        "outputId": "d2b67d96-dd66-4cf6-cead-5bab3400045d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl (7.1 MB)\n",
            "     ---------------------------------------- 7.1/7.1 MB 41.5 MB/s eta 0:00:00\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from scikit-learn) (1.21.6)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.0.2 threadpoolctl-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIayySkymEUe",
        "outputId": "5462ac6c-65f4-4561-badf-4f7a01a73889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openpyxl\n",
            "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
            "Collecting et-xmlfile\n",
            "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgjvhFyomEUe",
        "outputId": "62fc3586-ef0d-47dc-a257-f3b80455adae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Requirement already satisfied: joblib in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: click in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: importlib-metadata in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from click->nltk) (6.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from importlib-metadata->click->nltk) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\sanje\\anaconda3\\envs\\chatbot_37\\lib\\site-packages (from importlib-metadata->click->nltk) (4.3.0)\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5LDCQHRmEUe"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import chatterbot.corpus\n",
        "from chatterbot import comparisons\n",
        "from chatterbot import response_selection\n",
        "from chatterbot import ChatBot\n",
        "from chatterbot.comparisons import LevenshteinDistance\n",
        "from chatterbot.response_selection import get_first_response\n",
        "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
        "from chatterbot.trainers import ListTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import sklearn\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import glob\n",
        "import os\n",
        "import nltk\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# first-time use only\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# first-time use only\n",
        "# nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6lCbRfPmEUf"
      },
      "outputs": [],
      "source": [
        "def faq_chatbot_initialize(chatbot_name, threshold=0.9, excel_path='data\\testforkb.xlsx', worksheet_name='FAQ'):\n",
        "    covid_faq_chatbot = ChatBot(\n",
        "        chatbot_name,\n",
        "        logic_adapters=[\n",
        "            {\n",
        "                \"import_path\": \"chatterbot.logic.BestMatch\",\n",
        "                \"statement_comparison_function\": LevenshteinDistance,\n",
        "                \"response_selection_method\": get_first_response,\n",
        "                \"maximum_similarity_threshold\": threshold\n",
        "            }\n",
        "        ],\n",
        "        preprocessors=[\n",
        "            'chatterbot.preprocessors.clean_whitespace'\n",
        "        ],\n",
        "        read_only=True,\n",
        "    )\n",
        "    trainer = ListTrainer(covid_faq_chatbot)\n",
        "    #trainer.train(\"chatterbot.corpus.english\")\n",
        "    # read questions and answers\n",
        "    data = pd.read_excel(excel_path, sheet_name=worksheet_name, engine='openpyxl')\n",
        "    question = data.get('Queston')\n",
        "    answer = data.get('Long_Answer')\n",
        "\n",
        "    #for i in range(0, 3):\n",
        "    #    print('[Q]', question[i], '\\n[A]', answer[i], '\\n\\n')\n",
        "\n",
        "    # Iteratively adding the question and answer\n",
        "    train_list = []\n",
        "    for i in range(len(question)):\n",
        "        train_list.append(question[i])\n",
        "        train_list.append(answer[i])\n",
        "    # train the faq\n",
        "    trainer.train(train_list)\n",
        "    return covid_faq_chatbot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdJxXQgBmEUf"
      },
      "outputs": [],
      "source": [
        "class NLP_Chatbot:\n",
        "\n",
        "    def __init__(self, name, file_path):\n",
        "        self.name = name\n",
        "        self.file_path = file_path\n",
        "        self.sents = self.generate_sents()\n",
        "        self.TfidfVec, self.tfidf = self.generate_tfidf()\n",
        "\n",
        "    def generate_sents(self):\n",
        "        raw = []\n",
        "        for filename in glob.glob(os.path.join(self.file_path, '*.txt')):\n",
        "            with open(os.path.join(os.getcwd(), filename), 'r') as f: # open in readonly mode\n",
        "                # do your stuff\n",
        "                lines = f.readlines()\n",
        "            raw.extend(lines)\n",
        "        sents = [ nltk.sent_tokenize(r) for r in raw ] # converts each paragraph to a list of sentences\n",
        "        sents = [ s for sent in sents for s in sent  ] # flatten the list\n",
        "        return sents\n",
        "\n",
        "    def generate_tfidf(self):\n",
        "\n",
        "        # prepare for lemmatization\n",
        "        WNL = nltk.stem.WordNetLemmatizer()\n",
        "        #for handling some known bugs while not using POS tag info.\n",
        "        exceptions = ['has', 'was', 'as', 'us', 'less']\n",
        "        def MyNormalize(text):\n",
        "            tokens=nltk.word_tokenize(text.lower())\n",
        "            tokens=[ t for t in tokens if t not in string.punctuation ]\n",
        "            toks = [WNL.lemmatize(t) if t not in exceptions else t for t in tokens  ]\n",
        "            return toks\n",
        "        # Prepare a preprocessing function that will do tokenization,\n",
        "        # case lowering, punctuation removal, and lemmatization\n",
        "        my_stop_words = text.ENGLISH_STOP_WORDS\n",
        "\n",
        "        # preprocess the sentences in data, remove stop words, and create a tf-idf vector\n",
        "        TfidfVec = TfidfVectorizer(tokenizer=MyNormalize, stop_words=my_stop_words)\n",
        "        tfidf = TfidfVec.fit_transform(self.sents)\n",
        "        return TfidfVec, tfidf\n",
        "\n",
        "    # function to match input to the preprocessed sentences\n",
        "    def get_response(self, user_response):\n",
        "        robo_response=''\n",
        "        new = self.TfidfVec.transform([user_response])\n",
        "        vals = cosine_similarity(new[0], self.tfidf)\n",
        "        idx=vals.argsort()[0][-1]\n",
        "        flat = vals.flatten()\n",
        "        flat.sort()\n",
        "        req_tfidf = flat[-1]\n",
        "        if(req_tfidf==0):\n",
        "            robo_response=robo_response+\"I am sorry! I don't understand you.\"\n",
        "            return robo_response\n",
        "        else:\n",
        "            robo_response = robo_response+self.sents[idx]\n",
        "            return robo_response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_SMVs1SmEUf"
      },
      "outputs": [],
      "source": [
        "def nlp_chatbot_initialize(chatbot_name,file_path='data/'):\n",
        "    chatbot = NLP_Chatbot(chatbot_name, file_path)\n",
        "    return chatbot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvAqIxFYmEUg"
      },
      "outputs": [],
      "source": [
        "def get_answer(faq_chatbot, nlp_chatbot, question, threshold):  # let's get a response to our input\n",
        "    # try suggested corpora to find best fit. If first corpus < theshold, try another.\n",
        "    # avoid random responses confidence 0\n",
        "    response = faq_chatbot.get_response(question)\n",
        "    if  response.confidence < threshold:  # not a good answer, look elsewhere\n",
        "        #response = nlp_chatbot.get_response(question)\n",
        "        response = '123'\n",
        "    else:\n",
        "        response = response.serialize()['text']\n",
        "    return response\n",
        "#import logginglogging.basicConfig(level=logging.INFO)    # Enable info level logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQww4GuXmEUg"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('C:\\\\Users\\\\sanje\\\\data\\\\testforkb.xlsx', sheet_name='FAQ', engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrai4CEQmEUg"
      },
      "outputs": [],
      "source": [
        "data.to_csv('df_output.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xS18C_OmEUg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a function to remove non-UTF characters\n",
        "def remove_non_utf(text):\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "# Apply the function to the entire DataFrame\n",
        "df_cleaned = data.applymap(remove_non_utf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-C0wnzimEUg"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6TE7vZImEUg"
      },
      "outputs": [],
      "source": [
        "df_cleaned.to_csv('df_output2.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80wGpX3zmEUg",
        "outputId": "bf1e7b1d-5682-45fe-cba7-756aad15c788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n"
          ]
        }
      ],
      "source": [
        "data_path = 'data/'\n",
        "excel_name =  'C:\\\\Users\\\\sanje\\\\data\\\\testforkb.xlsx'\n",
        "worksheet_name = 'FAQ'\n",
        "\n",
        "\n",
        "covid_faq_chatbot = faq_chatbot_initialize(\"Covid FAQ Chat Bot\", excel_path=excel_name, worksheet_name=worksheet_name)\n",
        "#covid_nlp_chatbot = nlp_chatbot_initialize(\"Covid NLP Chat Bot\", data_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-7CqxpamEUh",
        "outputId": "987a6cc3-bf37-4038-d02b-ec2ef11ad43c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:chatterbot.chatterbot:Beginning search for close text match\n",
            "INFO:chatterbot.chatterbot:Processing search results\n",
            "INFO:chatterbot.chatterbot:Similar text found: Shellfish 0.62\n",
            "INFO:chatterbot.chatterbot:Using \"Shellfish\" as a close match to \"fish\" with a confidence of 0.62\n",
            "INFO:chatterbot.chatterbot:Selecting response from 1 optimal responses.\n",
            "INFO:chatterbot.response_selection:Selecting first response from list of 1 options.\n",
            "INFO:chatterbot.chatterbot:Response selected. Using \"Option 6\"\n",
            "INFO:chatterbot.chatterbot:BestMatch selected \"Option 6\" as a response with a confidence of 0.62\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Option 6\n"
          ]
        }
      ],
      "source": [
        "threshold=0.3\n",
        "question = \"fish\"\n",
        "#question = \"what does Conversational AI refer to?\"\n",
        "\n",
        "print(get_answer(covid_faq_chatbot, covid_nlp_chatbot, question, threshold))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}